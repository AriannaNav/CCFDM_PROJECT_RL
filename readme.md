# CCFDM ‚Äì Curiosity Contrastive Forward Dynamics Model (SAC)

Riproduzione **fedele, modulare e sperimentale** del paper  
**‚ÄúCuriosity Contrastive Forward Dynamics Model (CCFDM)‚Äù**,  
implementata in **PyTorch**, utilizzando **Soft Actor-Critic (SAC)** come algoritmo RL di base  
(**NON PPO**).

L‚Äôobiettivo del progetto √®:
- riprodurre **esattamente l‚Äôalgoritmo del paper**
- mantenere una **struttura pulita e modulare**
- supportare **ambienti diversi** (GridWorld ‚Üí MiniGrid ‚Üí DMC)
- permettere **ablation study, confronti e generalizzazione**

---
---

## üîπ agents/

### agents/sac.py

**Responsabilit√†**
- Implementazione **pura** di Soft Actor-Critic
- **NON** contiene CCFDM
- Serve come:
  - baseline sperimentale
  - modulo riutilizzato da `ccfdm_agent.py`

**Contenuto**
- update critic (2 Q-networks)
- update actor
- update temperatura Œ±
- soft update dei target networks
- logging di:
  - actor loss
  - critic loss
  - entropy
  - Œ±

‚ö†Ô∏è **Non deve sapere nulla di contrastive learning o curiosit√†**

---

### agents/ccfdm_agent.py

## üß† CUORE DEL PAPER

Questo file orchestra **l‚Äôintero algoritmo CCFDM**  
(**Algorithm 1 del paper**).

**Responsabilit√†**
- sampling dal replay buffer
- data augmentation
- encoding:
  - Query Encoder (QE)
  - Key Encoder (KE, aggiornato via EMA)
- Forward Dynamics Model (FDM)
- loss contrastiva (InfoNCE)
- calcolo intrinsic reward (Eq. 9)
- combinazione reward estrinseco + intrinseco
- chiamata agli update SAC

---

### üîÅ Pipeline algoritmica (step-by-step)

1. **Sample batch**
2. **Data augmentation**
3. **Encoding**
- q = QE(oÃÇ_t)
- k = KE(oÃÇ_t)
- k‚Å∫ = KE(oÃÇ_{t+1})

4. **Predizione dinamica**
- q' = FDM(q, AE(a_t))

5. **Loss contrastiva (InfoNCE)**
- positiva: (q', k‚Å∫)
- negative: altri sample nel batch

6. **Intrinsic reward**
- errore FDM
- normalizzazione
- decay temporale

7. **Reward finale**
8. **Update**
- encoder
- action embedding
- FDM
- SAC (actor, critic)

9. **Update EMA**
---

## üîπ data/

### data/replay_buffer.py

**Responsabilit√†**
- replay buffer **unico per tutti gli env**
- supporto a:
- immagini
- azioni continue
- reward
- obs_next
- supporto batch per contrastive learning:
- anchor
- positive
- negative (implicitamente il batch)

‚ö†Ô∏è **NON inserire logica di training qui**

---

### data/augmentations.py

**Responsabilit√†**
- data augmentation per immagini:
- random crop
- shift
- color jitter (opzionale)
- deve essere usata **solo per contrastive learning**
- **NON** va usata per `env.step`

---

## üîπ envs/

### envs/make_env.py

**Factory centrale degli ambienti**

Qui si decide:
- quale env usare (dmc, minigrid, gridworld)
- wrapper comuni
- output standardizzato

**Output obbligatorio per TUTTI gli env**
- obs: uint8 `[C, 84, 84]`
- action: float32 (anche se discreto internamente)
- reward: float
- done: bool

üëâ Questo √® ci√≤ che permette **un‚Äôunica codebase**.

---

### envs/dmc.py

Wrapper per:
- DeepMind Control Suite
- osservazioni pixel
- azioni continue

---

### envs/minigrid.py

Wrapper per:
- MiniGrid
- mapping azioni discrete ‚Üí continue
- rendering RGB
- frame stacking

---

## üîπ losses/

### losses/contrastive.py

**Loss InfoNCE (Eq. 8 del paper)**

**Responsabilit√†**
- costruzione logits
- similarit√† (dot product o bilinear)
- cross-entropy

‚ùå Questo file **non conosce** SAC, env o reward.

**Estendibile**
- BYOL
- SupCon
- altre loss contrastive

---

### losses/intrinsic.py

**Curiosity Module (Eq. 9 del paper)**

**Responsabilit√†**
- calcolo errore FDM
- normalizzazione (task-agnostic)
- decay temporale
- clipping

‚ùå NON deve accedere al replay buffer.

---

## üîπ models/

### models/encoder.py

Query Encoder / Key Encoder
- CNN per immagini
- output embedding `z`
- supporto `detach`
- KE aggiornato **solo via EMA** (no gradienti)

---

### models/action_embed.py

Action Embedding (AE)
- MLP
- a_t ‚Üí e(a_t)
- concatenazione con z_t

---

### models/fdm.py

Forward Dynamics Model (FDM)
- input: `[z_t, e(a_t)]`
- output: `zÃÇ_{t+1}`
- loss **implicita** tramite contrastive objective

---

### models/actor.py  
### models/critic.py

Architettura **SAC standard**.

---

## üîπ scripts/

### scripts/train_ccfdm.py

**Entry point principale**

**Responsabilit√†**
- parsing config
- creazione env
- creazione agent
- training loop
- evaluation periodica
- logging
- salvataggio modelli

‚ö†Ô∏è **QUI NON VA LOGICA ALGORITMICA**

---

### scripts/eval.py

Valutazione separata:
- return vs step
- sample efficiency (100k / 500k)
- state-space coverage
- policy stability

---

## üîπ utils/

### utils/ema.py

Aggiornamento EMA:
---

### utils/logger.py
- TensorBoard
- CSV / JSON
- logging centralizzato

---

### utils/seed.py
- riproducibilit√†
- torch
- numpy
- env

---

## üìä Metriche da implementare

### Sample Efficiency
- return vs environment steps
- score @ 100k, 500k

### State-Space Coverage
- embedding QE
- clustering / dispersione
- GridWorld: celle visitate
- DMC: entropy dell‚Äôembedding

### Policy Stability
- varianza ritorni
- entropy policy
- oscillazioni di Œ±

---

## üî¨ Estensioni previste

- nuove contrastive loss (`losses/`)
- nuovi env (`envs/`)
- ablation study:
  - no FDM
  - no intrinsic reward
  - CURL only
  - SAC only

---

## ‚úÖ Obiettivo finale

Una **riproduzione fedele del paper**, con:
- struttura chiara
- separazione netta dei ruoli
- generalizzazione cross-task
- confronti sperimentali solidi
