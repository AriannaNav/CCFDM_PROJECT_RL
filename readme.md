üîπ agents/

agents/sac.py

Responsabilit√†
	‚Ä¢	Implementazione pura di Soft Actor-Critic
	‚Ä¢	NON contiene CCFDM
	‚Ä¢	Serve come:
	‚Ä¢	baseline
	‚Ä¢	modulo riutilizzato da ccfdm_agent.py

Contenuto
	‚Ä¢	update critic (2 Q-networks)
	‚Ä¢	update actor
	‚Ä¢	update temperatura Œ±
	‚Ä¢	soft update target networks
	‚Ä¢	logging di:
	‚Ä¢	actor loss
	‚Ä¢	critic loss
	‚Ä¢	entropy
	‚Ä¢	Œ±

‚ö†Ô∏è Non deve sapere nulla di contrastive learning o curiosit√†

‚∏ª

agents/ccfdm_agent.py

CUORE DEL PAPER

Questo file orchestra l‚Äôintero algoritmo CCFDM (Algorithm 1 del paper).

Responsabilit√†
	‚Ä¢	sampling dal replay buffer
	‚Ä¢	data augmentation
	‚Ä¢	encoding:
	‚Ä¢	Query Encoder (QE)
	‚Ä¢	Key Encoder (KE, EMA)
	‚Ä¢	Forward Dynamics Model (FDM)
	‚Ä¢	loss contrastiva (InfoNCE)
	‚Ä¢	calcolo intrinsic reward (Eq. 9)
	‚Ä¢	combinazione reward estrinseco + intrinseco
	‚Ä¢	chiamata a SAC update

Pipeline da implementare (step-by-step)
	1.	Sample batch B = (o_t, a_t, o_{t+1}, r_t)
	2.	Applica augmentation ‚Üí BÃÇ
	3.	Calcola:
	‚Ä¢	q = QE(oÃÇ_t)
	‚Ä¢	k = KE(oÃÇ_t)
	‚Ä¢	k‚Å∫ = KE(oÃÇ_{t+1})
	4.	Predizione dinamica:
	‚Ä¢	q' = FDM(q, AE(a_t))
	5.	Loss contrastiva InfoNCE:
	‚Ä¢	positiva: (q', k‚Å∫)
	‚Ä¢	negative: batch
	6.	Intrinsic reward:
	‚Ä¢	errore FDM
	‚Ä¢	normalizzazione
	‚Ä¢	decay temporale
	7.	Reward finale:
    r_total = r_ext + C * exp(-Œ≥t) * r_int
    8.	Update:
	‚Ä¢	encoder
	‚Ä¢	action embedding
	‚Ä¢	FDM
	‚Ä¢	SAC (actor, critic)
	9.	Update EMA:
	‚Ä¢	KE ‚Üê œÑ¬∑QE + (1‚àíœÑ)¬∑KE
    üîπ data/

data/replay_buffer.py

Responsabilit√†
	‚Ä¢	replay buffer unico per tutti gli env
	‚Ä¢	supporto:
	‚Ä¢	immagini
	‚Ä¢	azioni continue
	‚Ä¢	rewards
	‚Ä¢	obs_next
	‚Ä¢	supporto a batch per contrastive learning
	‚Ä¢	anchor
	‚Ä¢	positive
	‚Ä¢	negative (implicitamente batch)

‚ö†Ô∏è NON inserire logica di training qui.

data/augmentations.py

Responsabilit√†
	‚Ä¢	data augmentation per immagini:
	‚Ä¢	random crop
	‚Ä¢	shift
	‚Ä¢	color jitter (opzionale)
	‚Ä¢	deve essere usata solo per contrastive learning, non per env.step

üîπ envs/

envs/make_env.py

Factory centrale degli ambienti

Qui si decide:
	‚Ä¢	quale env usare (dmc, minigrid, gridworld)
	‚Ä¢	wrapper comuni
	‚Ä¢	output standardizzato

Output obbligatorio per TUTTI gli env
	‚Ä¢	obs: uint8 [C, 84, 84]
	‚Ä¢	action: float32 (anche se discreto internamente)
	‚Ä¢	reward: float
	‚Ä¢	done

Questo √® ci√≤ che permette un‚Äôunica codebase.

envs/dmc.py

Wrapper per:
	‚Ä¢	DeepMind Control Suite
	‚Ä¢	pixel observations
	‚Ä¢	continuous actions

envs/minigrid.py

Wrapper per:
	‚Ä¢	MiniGrid
	‚Ä¢	mapping azioni discrete ‚Üí continue
	‚Ä¢	rendering RGB
	‚Ä¢	frame stacking

üîπ losses/

losses/contrastive.py

Loss InfoNCE (Eq. 8 del paper)

Responsabilit√†:
	‚Ä¢	costruzione logits
	‚Ä¢	similarit√† (dot o bilinear)
	‚Ä¢	cross-entropy

Questo file non conosce SAC, env, reward.

Estendibile:
	‚Ä¢	puoi aggiungere altre loss (BYOL, SupCon, ecc.)

‚∏ª

losses/intrinsic.py

Curiosity Module (Eq. 9)

Responsabilit√†:
	‚Ä¢	calcolo errore FDM
	‚Ä¢	normalizzazione (task-agnostic)
	‚Ä¢	decay temporale
	‚Ä¢	clipping

NON deve accedere al replay buffer.

‚∏ª

üîπ models/

models/encoder.py

Query Encoder / Key Encoder
	‚Ä¢	CNN per immagini
	‚Ä¢	output embedding z
	‚Ä¢	supporto detach
	‚Ä¢	KE viene aggiornato via EMA (non gradienti)

‚∏ª

models/action_embed.py

Action Embedding (AE)
	‚Ä¢	MLP
	‚Ä¢	a_t ‚Üí e(a_t)
	‚Ä¢	concat con z_t

‚∏ª

models/fdm.py

Forward Dynamics Model (FDM)
	‚Ä¢	input: [z_t, e(a_t)]
	‚Ä¢	output: zÃÇ_{t+1}
	‚Ä¢	loss: implicita via contrastive objective

‚∏ª

models/actor.py, models/critic.py

Architettura SAC standard.

‚∏ª

üîπ scripts/

scripts/train_ccfdm.py

Entry point principale

Responsabilit√†:
	‚Ä¢	parsing config
	‚Ä¢	creazione env
	‚Ä¢	creazione agent
	‚Ä¢	training loop
	‚Ä¢	evaluation periodica
	‚Ä¢	logging
	‚Ä¢	salvataggio modelli

‚ö†Ô∏è Qui NON va logica algoritmica, solo orchestrazione.

‚∏ª

scripts/eval.py

Valutazione separata:
	‚Ä¢	return vs step
	‚Ä¢	sample efficiency (100k / 500k)
	‚Ä¢	state-space coverage (embedding-based)
	‚Ä¢	policy stability (varianza ritorni)

‚∏ª

üîπ utils/

utils/ema.py

Aggiornamento:
Œ∏_k ‚Üê œÑ Œ∏_q + (1 ‚àí œÑ) Œ∏_k

utils/logger.py
	‚Ä¢	TensorBoard
	‚Ä¢	CSV / JSON
	‚Ä¢	logging centralizzato

‚∏ª

utils/seed.py

Riproducibilit√†:
	‚Ä¢	torch
	‚Ä¢	numpy
	‚Ä¢	env

‚∏ª

üìä Metriche da implementare

Sample Efficiency
	‚Ä¢	return vs environment steps
	‚Ä¢	score @ 100k, 500k

State-Space Coverage
	‚Ä¢	embedding QE
	‚Ä¢	clustering o dispersione
	‚Ä¢	GridWorld: celle visitate
	‚Ä¢	DMC: embedding entropy

Policy Stability
	‚Ä¢	varianza ritorni
	‚Ä¢	entropy policy
	‚Ä¢	oscillazioni Œ±

‚∏ª

üî¨ Estensioni previste
	‚Ä¢	nuove contrastive loss (losses/)
	‚Ä¢	nuovi env (envs/)
	‚Ä¢	ablation:
	‚Ä¢	no FDM
	‚Ä¢	no intrinsic reward
	‚Ä¢	CURL only
	‚Ä¢	SAC only

‚∏ª

‚úÖ Obiettivo finale

Una riproduzione fedele del paper, con:
	‚Ä¢	struttura chiara
	‚Ä¢	estendibilit√†
	‚Ä¢	confronti scientifici
	‚Ä¢	generalizzazione cross-task
