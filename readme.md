


## ğŸ“ Struttura del progetto (flat)

Il progetto utilizza una **struttura flat** (tutti file Python allo stesso livello)  
per evitare problemi di import e mantenere la massima semplicitÃ  operativa.
CCFDM_PROJECT_RL/
â”‚
â”œâ”€â”€ train_ccfdm.py      # entry point del training
â”œâ”€â”€ eval.py             # valutazione e metriche
â”‚
â”œâ”€â”€ ccfdm_agent.py      # algoritmo CCFDM (CUORE DEL PAPER)
â”œâ”€â”€ sac.py              # Soft Actor-Critic puro (baseline)
â”‚
â”œâ”€â”€ encoder.py          # Query / Key Encoder
â”œâ”€â”€ models.py           # Actor, Critic, Action Embedding, FDM
â”œâ”€â”€ losses.py           # InfoNCE + Curiosity (intrinsic reward)
â”‚
â”œâ”€â”€ data.py             # replay buffer + data augmentation
â”‚
â”œâ”€â”€ make_env.py         # factory ambienti
â”œâ”€â”€ dmc.py              # wrapper DeepMind Control Suite
â”œâ”€â”€ minigrid.py         # wrapper MiniGrid
â”‚
â”œâ”€â”€ utils.py            # seed, device, helper comuni
â”œâ”€â”€ logger.py           # logging centralizzato
â”‚
â””â”€â”€ readme.md

---

## ğŸ§  Algoritmo CCFDM

### `ccfdm_agent.py` â€” **CUORE DEL PAPER**

Questo file implementa **lâ€™intero algoritmo CCFDM**  
cosÃ¬ come descritto nel paper (**Algorithm 1**).

### ResponsabilitÃ 
- sampling dal replay buffer
- data augmentation
- encoding:
  - Query Encoder (QE)
  - Key Encoder (KE, aggiornato via EMA)
- Forward Dynamics Model (FDM)
- loss contrastiva (InfoNCE)
- calcolo intrinsic reward (Eq. 9)
- combinazione reward estrinseco + intrinseco
- chiamata agli update SAC

---

### ğŸ” Pipeline algoritmica (step-by-step)

1. **Sample batch**
2. **Data augmentation**
3. **Encoding**
- q = QE(Ã´_t)
- k = KE(Ã´_t)
- kâº = KE(Ã´_{t+1})

4. **Predizione dinamica**
- q' = FDM(q, AE(a_t))

5. **Loss contrastiva (InfoNCE, Eq. 8)**
- positiva: (q', kâº)
- negative: altri sample nel batch

6. **Intrinsic reward (Eq. 9)**
- errore del Forward Dynamics Model
- normalizzazione task-agnostic
- decay temporale

7. **Reward finale**
8. **Update**
- encoder
- action embedding
- FDM
- SAC (actor + critic)

9. **Update EMA**
---

## ğŸ¤– Reinforcement Learning

### `sac.py`

Implementazione **pura** di Soft Actor-Critic.
-ACTOR 
-CRITIC

**Caratteristiche**
- 2 Q-networks
- update actor
- update temperatura Î±
- soft update dei target network

âš ï¸ **Non conosce nulla di CCFDM, curiositÃ  o contrastive learning**  
Serve come:
- baseline
- componente riutilizzata da `ccfdm_agent.py`

---

## ğŸ§© Modelli

### `encoder.py`
- CNN per osservazioni RGB
- produce embedding latente `z`
- supporto `detach`
- Query Encoder (QE)
- Key Encoder (KE) aggiornato solo via EMA

---

### `models.py`
Contiene:
- Action Embedding (AE)
- Forward Dynamics Model (FDM)

Il FDM:
- input: `[z_t, e(a_t)]`
- output: `zÌ‚_{t+1}`
- supervisione **implicita** tramite loss contrastiva

---


## ğŸ“‰ Loss e CuriositÃ 

### `losses.py`

Contiene:
- **InfoNCE** (Eq. 8)
- **Curiosity Module** (Eq. 9)

ResponsabilitÃ :
- costruzione logits contrastivi
- similaritÃ  (dot / bilinear)
- cross-entropy
- errore FDM
- normalizzazione e decay temporale

âŒ Non conosce SAC nÃ© il replay buffer.

---

## ğŸ›  Utility

---

### `logger.py`
- logging centralizzato
- scalari
- supporto TensorBoard (opzionale)

---

## ğŸš€ Training ed Evaluation

### `train_ccfdm.py`
Entry point principale.

ResponsabilitÃ :
- parsing argomenti
- setup device e seed
- creazione env
- creazione agent
- training loop
- evaluation periodica
- salvataggio modelli

âš ï¸ **Nessuna logica algoritmica qui**

---

### `eval.py`
Valutazione separata:
- return vs environment steps
- sample efficiency (100k / 500k)
- state-space coverage
- policy stability

---

## ğŸ”§ Training parametrico e sperimentazione

Il progetto Ã¨ progettato per consentire **esperimenti controllati** senza riscrivere codice.

Ãˆ possibile:
- cambiare ambiente (GridWorld â†’ MiniGrid â†’ DMC)
- cambiare loss contrastiva
- disattivare curiositÃ  o FDM
- confrontare:
- SAC only
- CURL only
- CCFDM completo

Il file `ccfdm_agent.py` rimane **immutato**:  
le variazioni avvengono **per composizione**, non per riscrittura.

---

## ğŸ“Š Metriche di analisi

- **Sample Efficiency**
- return vs steps
- score @ 100k / 500k
- **State-Space Coverage**
- dispersione embedding QE
- celle visitate (GridWorld)
- entropy embedding (DMC)
- **Policy Stability**
- varianza dei ritorni
- entropy della policy
- oscillazioni di Î±

---

## âœ… Obiettivo finale

Una **riproduzione fedele del paper CCFDM**, con:
- codice leggibile
- struttura coerente
- generalizzazione cross-task
- base solida per ricerca e tesi

## Cosa Installare
- pip install gymnasium minigrid
- pip install dm_control
- pip install pillow
- pip install opencv-python